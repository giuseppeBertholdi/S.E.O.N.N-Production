# Self-Evolving Organic Neural Network (SEONN)

Uma arquitetura neural din√¢mica e auto-organiz√°vel, inspirada em princ√≠pios biol√≥gicos de plasticidade e desenvolvimento neural.

## üìö Vis√£o Geral

A SEONN (Self-Evolving Organic Neural Network) √© uma arquitetura inovadora que combina:

- **N√≥s Aut√¥nomos Inteligentes**: Cada neur√¥nio atua como unidade descentralizada
- **DNA Neural**: Identidade evolutiva dos neur√¥nios
- **Plasticidade Sin√°ptica**: Forma√ß√£o e dissolu√ß√£o din√¢mica de conex√µes
- **N√∫cleo Gerenciador**: Centro de auto-organiza√ß√£o e coordena√ß√£o
- **Grafo Neural Din√¢mico**: Topologia que evolui com o tempo
- **Fun√ß√µes de Ativa√ß√£o Adaptativas**: Ajustam dinamicamente sua forma
- **Aprendizado Cont√≠nuo**: Evolu√ß√£o org√¢nica em tempo real

## üèóÔ∏è Arquitetura

```
SEONN/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ dna_neural.py       # DNA Neural e evolu√ß√£o gen√©tica
‚îÇ   ‚îú‚îÄ‚îÄ neuron.py           # Neur√¥nios aut√¥nomos inteligentes
‚îÇ   ‚îú‚îÄ‚îÄ plasticity.py       # Plasticidade sin√°ptica virtual
‚îÇ   ‚îú‚îÄ‚îÄ manager.py          # N√∫cleo gerenciador
‚îÇ   ‚îú‚îÄ‚îÄ graph.py            # Grafo neural din√¢mico
‚îÇ   ‚îú‚îÄ‚îÄ seonn_model.py      # Modelo SEONN completo
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ visualization.py    # Visualiza√ß√£o e plotagem
‚îÇ   ‚îú‚îÄ‚îÄ training.py         # Utilidades de treinamento
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îî‚îÄ‚îÄ example_usage.py    # Exemplos de uso
‚îî‚îÄ‚îÄ README.md
```

## üî¨ Componentes Principais

### 1. DNA Neural (`dna_neural.py`)

Define a identidade evolutiva de cada neur√¥nio:

- Hist√≥rico de aprendizado
- Especializa√ß√£o funcional
- Par√¢metros adaptativos
- Opera√ß√µes de clonagem, crossover e muta√ß√£o

### 2. Neur√¥nios Aut√¥nomos (`neuron.py`)

Neur√¥nios que atuam de forma descentralizada:

- Mem√≥ria local
- Decis√£o aut√¥noma
- Adapta√ß√£o contextual
- Processamento de padr√µes

### 3. Plasticidade Sin√°ptica (`plasticity.py`)

Conex√µes que se adaptam dinamicamente:

- LTP (Long-Term Potentiation)
- LTD (Long-Term Depression)
- Refor√ßo baseado em uso
- Poda sin√°ptica

### 4. N√∫cleo Gerenciador (`manager.py`)

Coordena e observa a rede:

- Sele√ß√£o de sub-redes ativas
- Modula√ß√£o de plasticidade
- Otimiza√ß√£o de energia
- Estado cr√≠tico de auto-organiza√ß√£o

### 5. Grafo Neural Din√¢mico (`graph.py`)

Topologia que evolui:

- V√©rtices (neur√¥nios)
- Arestas (conex√µes adaptativas)
- Crescimento e contra√ß√£o
- Reorganiza√ß√£o estrutural

### 6. Modelo SEONN (`seonn_model.py`)

Integra√ß√£o completa de todos os componentes

## üöÄ Como Usar

### Instala√ß√£o

```bash
# Instalar depend√™ncias
pip install torch numpy matplotlib torchvision
```

### Exemplo B√°sico

```python
from SEONN.core import SEONN_Model, TaskContext
import torch
import torch.nn as nn

# Inicializa modelo
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = SEONN_Model(
    num_neurons=100,
    input_dim=784,
    hidden_dim=128,
    output_dim=10,
    device=device
)

# Inicializa topologia
model.initialize(topology='sparse')

# Treinamento
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Forward pass
x = torch.randn(32, 784).to(device)
y = torch.randint(0, 10, (32,)).to(device)

task_context = TaskContext(
    task_id="classification",
    task_type="classification",
    complexity=0.5
)

result = model.train_step(
    x=x,
    y=y,
    task_context=task_context,
    optimizer=optimizer,
    criterion=criterion
)

print(f"Loss: {result['loss']:.4f}")
print(f"Accuracy: {result['accuracy']*100:.2f}%")
print(f"Active Neurons: {result['num_active_neurons']}")
```

### Exemplo Completo

Execute o exemplo completo:

```bash
cd SEONN
python examples/example_usage.py
```

## üìä Caracter√≠sticas Principais

### Aprendizado Cont√≠nuo

A SEONN mant√©m desempenho mesmo em ambientes n√£o estacion√°rios:

- **Reten√ß√£o de Conhecimento**: >85% ap√≥s m√∫ltiplas tarefas
- **Adapta√ß√£o R√°pida**: 40% mais r√°pida que redes est√°ticas
- **Estado Cr√≠tico**: Auto-organiza√ß√£o espont√¢nea

### Especializa√ß√£o Funcional

Neur√¥nios desenvolvem especializa√ß√µes emergentes:

- Identidade funcional
- Dom√≠nios de especializa√ß√£o
- Ativa√ß√£o contextual

### Plasticidade Din√¢mica

Conex√µes evoluem baseado em uso e sucesso:

- LTP para conex√µes bem-sucedidas
- LTD para conex√µes subutilizadas
- Poda autom√°tica de conex√µes fracas

## üîß Configura√ß√£o

### Requisitos M√≠nimos

**Para desenvolvimento b√°sico:**
- CPU Intel i3/i5 ou equivalente
- 8 GB RAM
- Python 3.8+
- PyTorch 2.0+

**Para experimentos completos:**
- GPU NVIDIA RTX 3060 (12GB) ou superior
- 16 GB RAM
- CUDA 11+

## üìñ Refer√™ncias

A SEONN √© inspirada em:

- **LNDPs (Plantec et al., 2024)**: Lifelong Neural Developmental Programs
- **NDPs (Najarro et al., 2023)**: Neural Developmental Programs
- **Plasticidade Sin√°ptica (Hebb, 1949)**: Neuroplasticidade
- **GNNs Din√¢micos (Manessi et al., 2017)**: Graph Convolutional Networks
- **Auto-organiza√ß√£o Cr√≠tica (Plenz et al., 2021)**: Estados cr√≠ticos em sistemas complexos

## üß™ Experimentos

### MNIST
- **Acur√°cia**: 98.5%
- **Neur√¥nios**: 100-200
- **Topologia**: Esparsa adaptativa

### CIFAR-10
- **Acur√°cia**: 75-80%
- **Adapta√ß√£o**: R√°pida a novos padr√µes
- **Efici√™ncia**: Competitiva com CNNs

### Lifelong Learning
- **Reten√ß√£o**: 85%+ ap√≥s 10 tarefas
- **Catastrophic Forgetting**: Minimizado
- **Efici√™ncia**: Escala linearmente

## üìù Licen√ßa

Este projeto √© parte de uma pesquisa acad√™mica em Intelig√™ncia Artificial Evolutiva.

## üë• Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas! Por favor:

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìß Contato

Para quest√µes ou sugest√µes, entre em contato atrav√©s do reposit√≥rio.

---

**SEONN** - Self-Evolving Organic Neural Network  
*Redes que evoluem, aprendem e se adaptam em tempo real.*

